{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning using EKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing Pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: D:\\Projects\\NLP_Project\\NeuralNetworkswKalmanFilters\\src\n",
      "The data directory is ..\\data\n"
     ]
    }
   ],
   "source": [
    "# Select data directory\n",
    "print(\"Current Directory:\",os.getcwd())\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('..\\data/'):\n",
    "    course_data_dir = '..\\data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    # course_data_dir = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Select device which you are going to use for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data Sets\n",
    "Testing using a toy sine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let us generate toy data\n",
    "# np.random.seed(2)\n",
    "# x_train = np.random.randn(100, 1)\n",
    "# x_train = np.sort(x_train, axis=0)\n",
    "# y_train = np.sin(x_train * 2 * np.pi / 3)\n",
    "# y_train = y_train + 0.2 * np.random.randn(*y_train.shape)\n",
    "\n",
    "# # Plot the data\n",
    "# fig, ax = plt.subplots(1)\n",
    "# ax.plot(x_train, y_train, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in ..\\data\\fashion_mnist\n"
     ]
    }
   ],
   "source": [
    "## Use MNIST data set\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(course_data_dir, 'fashion_mnist')\n",
    "print('Data stored in %s' % data_dir)\n",
    "trainset = torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "           'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "batch_size = 32\n",
    "test_batch_size = 5\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, n_rows=1):\n",
    "    fig, axs = plt.subplots(n_rows, images.size(0) // n_rows)\n",
    "    for ax, img in zip(axs.flat, images):\n",
    "        ax.matshow(img[0].cpu().numpy(), cmap=plt.cm.Greys)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    plt.tight_layout(w_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2247a385c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "plot_images(images[:8], n_rows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the accuracy on the test dataset\n",
    "def compute_accuracy(test_net, testloader):\n",
    "    test_net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            #print(images.size())\n",
    "            outputs = test_net(images.view(-1,784))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# x_train_scaled = scaler.fit_transform(x_train)\n",
    "# x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_layer, n_outputs,bias=True):\n",
    "        super(MLP, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        self.fc1 = nn.Linear(n_inputs, n_hidden_layer, bias)\n",
    "        self.fc2 = nn.Linear(n_hidden_layer, n_hidden_layer, bias)\n",
    "        self.fc3 = nn.Linear(n_hidden_layer, n_outputs, bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=1, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Testing net\n",
    "n_inputs = 1\n",
    "n_outputs = 1\n",
    "n_hidden_layer = 100\n",
    "test_net = MLP(n_inputs, n_hidden_layer, n_outputs)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "n_inputs = 784\n",
    "n_outputs = 10\n",
    "n_hidden_layer = 32\n",
    "net_bp = MLP(n_inputs, n_hidden_layer,n_outputs)\n",
    "net_bp.to(device)\n",
    "#\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_bp.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 2.130\n",
      "[1,   400] loss: 1.647\n",
      "[1,   600] loss: 1.256\n",
      "[1,   800] loss: 1.071\n",
      "[1,  1000] loss: 0.952\n",
      "[1,  1200] loss: 1.056\n",
      "[1,  1400] loss: 0.915\n",
      "[1,  1600] loss: 0.744\n",
      "[1,  1800] loss: 0.815\n",
      "[1,  2000] loss: 0.795\n",
      "[1,  2200] loss: 0.855\n",
      "[1,  2400] loss: 0.834\n",
      "[1,  2600] loss: 0.758\n",
      "[1,  2800] loss: 0.759\n",
      "[1,  3000] loss: 0.917\n",
      "[1,  3200] loss: 0.808\n",
      "[1,  3400] loss: 0.735\n",
      "[1,  3600] loss: 0.741\n",
      "[1,  3800] loss: 0.606\n",
      "[1,  4000] loss: 0.680\n",
      "[1,  4200] loss: 0.733\n",
      "[1,  4400] loss: 0.682\n",
      "[1,  4600] loss: 0.672\n",
      "[1,  4800] loss: 0.827\n",
      "[1,  5000] loss: 0.698\n",
      "[1,  5200] loss: 0.721\n",
      "[1,  5400] loss: 0.646\n",
      "[1,  5600] loss: 0.639\n",
      "[1,  5800] loss: 0.690\n",
      "[1,  6000] loss: 0.780\n",
      "[1,  6200] loss: 0.773\n",
      "[1,  6400] loss: 0.590\n",
      "[1,  6600] loss: 0.656\n",
      "[1,  6800] loss: 0.601\n",
      "[1,  7000] loss: 0.580\n",
      "[1,  7200] loss: 0.625\n",
      "[1,  7400] loss: 0.613\n",
      "[1,  7600] loss: 0.572\n",
      "[1,  7800] loss: 0.673\n",
      "[1,  8000] loss: 0.775\n",
      "[1,  8200] loss: 0.766\n",
      "[1,  8400] loss: 0.762\n",
      "[1,  8600] loss: 0.723\n",
      "[1,  8800] loss: 0.564\n",
      "[1,  9000] loss: 0.657\n",
      "[1,  9200] loss: 0.572\n",
      "[1,  9400] loss: 0.615\n",
      "[1,  9600] loss: 0.612\n",
      "[1,  9800] loss: 0.590\n",
      "[1, 10000] loss: 0.651\n",
      "[1, 10200] loss: 0.654\n",
      "[1, 10400] loss: 0.499\n",
      "[1, 10600] loss: 0.546\n",
      "[1, 10800] loss: 0.592\n",
      "[1, 11000] loss: 0.575\n",
      "[1, 11200] loss: 0.573\n",
      "[1, 11400] loss: 0.561\n",
      "[1, 11600] loss: 0.513\n",
      "[1, 11800] loss: 0.643\n",
      "[1, 12000] loss: 0.651\n",
      "[1, 12200] loss: 0.565\n",
      "[1, 12400] loss: 0.670\n",
      "[1, 12600] loss: 0.608\n",
      "[1, 12800] loss: 0.715\n",
      "[1, 13000] loss: 0.607\n",
      "[1, 13200] loss: 0.483\n",
      "[1, 13400] loss: 0.604\n",
      "[1, 13600] loss: 0.705\n",
      "[1, 13800] loss: 0.522\n",
      "[1, 14000] loss: 0.565\n",
      "[1, 14200] loss: 0.628\n",
      "[1, 14400] loss: 0.388\n",
      "[1, 14600] loss: 0.546\n",
      "[1, 14800] loss: 0.565\n",
      "[1, 15000] loss: 0.495\n",
      "[1, 15200] loss: 0.473\n",
      "[1, 15400] loss: 0.626\n",
      "[1, 15600] loss: 0.568\n",
      "[1, 15800] loss: 0.443\n",
      "[1, 16000] loss: 0.763\n",
      "[1, 16200] loss: 0.656\n",
      "[1, 16400] loss: 0.535\n",
      "[1, 16600] loss: 0.513\n",
      "[1, 16800] loss: 0.499\n",
      "[1, 17000] loss: 0.564\n",
      "[1, 17200] loss: 0.525\n",
      "[1, 17400] loss: 0.534\n",
      "[1, 17600] loss: 0.573\n",
      "[1, 17800] loss: 0.657\n",
      "[1, 18000] loss: 0.542\n",
      "[1, 18200] loss: 0.484\n",
      "[1, 18400] loss: 0.494\n",
      "[1, 18600] loss: 0.493\n",
      "[1, 18800] loss: 0.552\n",
      "[1, 19000] loss: 0.520\n",
      "[1, 19200] loss: 0.608\n",
      "[1, 19400] loss: 0.457\n",
      "[1, 19600] loss: 0.523\n",
      "[1, 19800] loss: 0.534\n",
      "[1, 20000] loss: 0.585\n",
      "[1, 20200] loss: 0.438\n",
      "[1, 20400] loss: 0.504\n",
      "[1, 20600] loss: 0.522\n",
      "[1, 20800] loss: 0.621\n",
      "[1, 21000] loss: 0.624\n",
      "[1, 21200] loss: 0.554\n",
      "[1, 21400] loss: 0.580\n",
      "[1, 21600] loss: 0.519\n",
      "[1, 21800] loss: 0.531\n",
      "[1, 22000] loss: 0.361\n",
      "[1, 22200] loss: 0.530\n",
      "[1, 22400] loss: 0.543\n",
      "[1, 22600] loss: 0.451\n",
      "[1, 22800] loss: 0.479\n",
      "[1, 23000] loss: 0.664\n",
      "[1, 23200] loss: 0.469\n",
      "[1, 23400] loss: 0.519\n",
      "[1, 23600] loss: 0.540\n",
      "[1, 23800] loss: 0.520\n",
      "[1, 24000] loss: 0.561\n",
      "[1, 24200] loss: 0.498\n",
      "[1, 24400] loss: 0.508\n",
      "[1, 24600] loss: 0.400\n",
      "[1, 24800] loss: 0.415\n",
      "[1, 25000] loss: 0.412\n",
      "[1, 25200] loss: 0.468\n",
      "[1, 25400] loss: 0.712\n",
      "[1, 25600] loss: 0.499\n",
      "[1, 25800] loss: 0.520\n",
      "[1, 26000] loss: 0.578\n",
      "[1, 26200] loss: 0.639\n",
      "[1, 26400] loss: 0.472\n",
      "[1, 26600] loss: 0.466\n",
      "[1, 26800] loss: 0.591\n",
      "[1, 27000] loss: 0.556\n",
      "[1, 27200] loss: 0.479\n",
      "[1, 27400] loss: 0.483\n",
      "[1, 27600] loss: 0.479\n",
      "[1, 27800] loss: 0.527\n",
      "[1, 28000] loss: 0.455\n",
      "[1, 28200] loss: 0.491\n",
      "[1, 28400] loss: 0.532\n",
      "[1, 28600] loss: 0.391\n",
      "[1, 28800] loss: 0.545\n",
      "[1, 29000] loss: 0.429\n",
      "[1, 29200] loss: 0.596\n",
      "[1, 29400] loss: 0.550\n",
      "[1, 29600] loss: 0.501\n",
      "[1, 29800] loss: 0.493\n",
      "[1, 30000] loss: 0.631\n",
      "[1, 30200] loss: 0.556\n",
      "[1, 30400] loss: 0.468\n",
      "[1, 30600] loss: 0.540\n",
      "[1, 30800] loss: 0.541\n",
      "[1, 31000] loss: 0.376\n",
      "[1, 31200] loss: 0.487\n",
      "[1, 31400] loss: 0.536\n",
      "[1, 31600] loss: 0.465\n",
      "[1, 31800] loss: 0.644\n",
      "[1, 32000] loss: 0.530\n",
      "[1, 32200] loss: 0.530\n",
      "[1, 32400] loss: 0.510\n",
      "[1, 32600] loss: 0.482\n",
      "[1, 32800] loss: 0.645\n",
      "[1, 33000] loss: 0.514\n",
      "[1, 33200] loss: 0.549\n",
      "[1, 33400] loss: 0.592\n",
      "[1, 33600] loss: 0.519\n",
      "[1, 33800] loss: 0.522\n",
      "[1, 34000] loss: 0.381\n",
      "[1, 34200] loss: 0.473\n",
      "[1, 34400] loss: 0.397\n",
      "[1, 34600] loss: 0.555\n",
      "[1, 34800] loss: 0.485\n",
      "[1, 35000] loss: 0.426\n",
      "[1, 35200] loss: 0.478\n",
      "[1, 35400] loss: 0.531\n",
      "[1, 35600] loss: 0.412\n",
      "[1, 35800] loss: 0.651\n",
      "[1, 36000] loss: 0.464\n",
      "[1, 36200] loss: 0.335\n",
      "[1, 36400] loss: 0.618\n",
      "[1, 36600] loss: 0.492\n",
      "[1, 36800] loss: 0.500\n",
      "[1, 37000] loss: 0.484\n",
      "[1, 37200] loss: 0.476\n",
      "[1, 37400] loss: 0.533\n",
      "[1, 37600] loss: 0.491\n",
      "[1, 37800] loss: 0.513\n",
      "[1, 38000] loss: 0.456\n",
      "[1, 38200] loss: 0.436\n",
      "[1, 38400] loss: 0.525\n",
      "[1, 38600] loss: 0.553\n",
      "[1, 38800] loss: 0.537\n",
      "[1, 39000] loss: 0.453\n",
      "[1, 39200] loss: 0.472\n",
      "[1, 39400] loss: 0.488\n",
      "[1, 39600] loss: 0.435\n",
      "[1, 39800] loss: 0.439\n",
      "[1, 40000] loss: 0.440\n",
      "[1, 40200] loss: 0.560\n",
      "[1, 40400] loss: 0.461\n",
      "[1, 40600] loss: 0.649\n",
      "[1, 40800] loss: 0.548\n",
      "[1, 41000] loss: 0.537\n",
      "[1, 41200] loss: 0.427\n",
      "[1, 41400] loss: 0.476\n",
      "[1, 41600] loss: 0.475\n",
      "[1, 41800] loss: 0.392\n",
      "[1, 42000] loss: 0.504\n",
      "[1, 42200] loss: 0.415\n",
      "[1, 42400] loss: 0.411\n",
      "[1, 42600] loss: 0.590\n",
      "[1, 42800] loss: 0.503\n",
      "[1, 43000] loss: 0.407\n",
      "[1, 43200] loss: 0.449\n",
      "[1, 43400] loss: 0.413\n",
      "[1, 43600] loss: 0.531\n",
      "[1, 43800] loss: 0.580\n",
      "[1, 44000] loss: 0.499\n",
      "[1, 44200] loss: 0.402\n",
      "[1, 44400] loss: 0.566\n",
      "[1, 44600] loss: 0.405\n",
      "[1, 44800] loss: 0.432\n",
      "[1, 45000] loss: 0.460\n",
      "[1, 45200] loss: 0.477\n",
      "[1, 45400] loss: 0.483\n",
      "[1, 45600] loss: 0.572\n",
      "[1, 45800] loss: 0.543\n",
      "[1, 46000] loss: 0.495\n",
      "[1, 46200] loss: 0.451\n",
      "[1, 46400] loss: 0.464\n",
      "[1, 46600] loss: 0.445\n",
      "[1, 46800] loss: 0.412\n",
      "[1, 47000] loss: 0.356\n",
      "[1, 47200] loss: 0.570\n",
      "[1, 47400] loss: 0.674\n",
      "[1, 47600] loss: 0.388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-14df08c7743a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#         print(labels.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dle\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dle\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=10\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = 200  # mini-batches\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        # Transfer to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #print(inputs.shape)\n",
    "        inputs=inputs.view(-1,784)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net_bp(inputs)\n",
    "#         print(outputs.size())\n",
    "#         print(labels.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i % print_every) == (print_every-1):\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "#         if skip_training:\n",
    "#             break\n",
    "\n",
    "#     # Print accuracy after every epoch\n",
    "    accuracy = compute_accuracy(net_bp, testloader)\n",
    "    print('Accuracy of the network on the test images: %.3f' % accuracy)\n",
    "\n",
    "# #     if skip_training:\n",
    "# #         break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EKF for learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculate Weight size\n",
    "# weight_mat_size = 0\n",
    "# for i in range(len(layer_list)-1):\n",
    "#     weight_mat_size = weight_mat_size + (layer_list[i]*layer_list[i+1])\n",
    "        \n",
    "def getWeights(net):\n",
    "    weight_mat = []\n",
    "    for name,param in net.named_parameters():\n",
    "    \n",
    "        #print('Layer',name, 'is', param.data.shape)\n",
    "        if (len(list(param.data.shape)) == 2):\n",
    "            weight_mat.append(param.data.flatten())\n",
    "        \n",
    "    weight_mat = torch.cat(weight_mat, dim=0)       \n",
    "    #print('Shape of weight matrix', weight_mat.shape)\n",
    "    return weight_mat.view(-1, 1)\n",
    "\n",
    "def getWeightsgrad(net):\n",
    "    weight_grad_mat = []\n",
    "    for name,param in net.named_parameters():\n",
    "    \n",
    "        #print('Layer Grads',name, 'is', param.grad.shape)\n",
    "        if (len(list(param.grad.shape)) == 2):\n",
    "            weight_grad_mat.append(param.grad.flatten())\n",
    "        \n",
    "    weight_grad_mat = torch.cat(weight_grad_mat, dim=0)       \n",
    "    #print('Shape of weight matrix', weight_grad_mat.shape)   \n",
    "    return weight_grad_mat.view(-1, 1)\n",
    "\n",
    "def setWeights(net, weight_mat):\n",
    "    mem_ind = 0;\n",
    "    for name,param in net.named_parameters():\n",
    "        if (len(list(param.data.shape)) == 2):\n",
    "            #print('Layer',name, 'is', param.data.shape)\n",
    "            #print(torch.numel(param.data))\n",
    "            #print(weight_mat[mem_ind:mem_ind+torch.numel(param.data)].view(param.data.shape).shape)\n",
    "            param.data = weight_mat[mem_ind:mem_ind+torch.numel(param.data)].view(param.data.shape)\n",
    "            mem_ind = torch.numel(param.data)\n",
    "\n",
    "def L1ErrorSigned(outputs,labels):\n",
    "    \n",
    "    return (labels-outputs)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing network MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=10, bias=False)\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=False)\n",
      "  (fc3): Linear(in_features=10, out_features=10, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define number of Input and Output layers\n",
    "#device=torch.device(\"cpu\")\n",
    "device=torch.device(\"cuda:0\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "n_inputs = 784\n",
    "n_outputs=10\n",
    "n_hidden_layer = 10\n",
    "mlp_EKF = MLP(n_inputs,n_hidden_layer,n_outputs,bias = False)\n",
    "mlp_EKF = mlp_EKF.to(device)\n",
    "n_epochs = 10\n",
    "\n",
    "print(\"Printing network\",mlp_EKF)\n",
    "\n",
    "# Define EKF covariances\n",
    "# System Noise or also known as training  noise  \n",
    "Q = 1e-6*torch.eye((getWeights(mlp_EKF).shape[0]),device=device)\n",
    "# Measurement noise or noise in targets / Learning rate\n",
    "R = 10*torch.eye(batch_size*n_outputs,device=device)\n",
    "#Covariance Matrix\n",
    "P = 100*torch.eye((getWeights(mlp_EKF).shape[0]),device=device)\n",
    "#print(P.size())\n",
    "\n",
    "#print(weight_mat.shape) \n",
    "#print(x_train.shape)\n",
    "#print(y_train.shape)\n",
    "# Define loss function\n",
    "cross_err_loss=nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.759\n",
      "[1,   100] loss: 0.818\n",
      "[1,   150] loss: 0.837\n",
      "[1,   200] loss: 0.825\n",
      "[1,   250] loss: 0.828\n",
      "[1,   300] loss: 0.826\n",
      "[1,   350] loss: 0.819\n",
      "[1,   400] loss: 0.821\n",
      "[1,   450] loss: 0.821\n",
      "[1,   500] loss: 0.786\n",
      "[1,   550] loss: 0.786\n",
      "[1,   600] loss: 0.622\n",
      "[1,   650] loss: 0.589\n",
      "[1,   700] loss: 0.570\n",
      "[1,   750] loss: 0.628\n",
      "[1,   800] loss: 0.623\n",
      "[1,   850] loss: 0.651\n",
      "[1,   900] loss: 0.657\n",
      "[1,   950] loss: 0.653\n",
      "[1,  1000] loss: 0.675\n",
      "[1,  1050] loss: 0.671\n",
      "[1,  1100] loss: 0.677\n",
      "[1,  1150] loss: 0.666\n",
      "[1,  1200] loss: 0.681\n",
      "[1,  1250] loss: 0.653\n",
      "[1,  1300] loss: 0.658\n",
      "[1,  1350] loss: 0.658\n",
      "[1,  1400] loss: 0.651\n",
      "[1,  1450] loss: 0.656\n",
      "[1,  1500] loss: 0.633\n",
      "[1,  1550] loss: 0.643\n",
      "[1,  1600] loss: 0.636\n",
      "[1,  1650] loss: 0.637\n",
      "[1,  1700] loss: 0.642\n",
      "[1,  1750] loss: 0.630\n",
      "[1,  1800] loss: 0.636\n",
      "[1,  1850] loss: 0.634\n",
      "Accuracy of the network on the test images: 0.119\n",
      "[2,    50] loss: 0.632\n",
      "[2,   100] loss: 0.620\n",
      "[2,   150] loss: 0.599\n",
      "[2,   200] loss: 0.585\n",
      "[2,   250] loss: 0.591\n",
      "[2,   300] loss: 0.610\n",
      "[2,   350] loss: 0.616\n",
      "[2,   400] loss: 0.580\n",
      "[2,   450] loss: 0.604\n",
      "[2,   500] loss: 0.588\n",
      "[2,   550] loss: 0.571\n",
      "[2,   600] loss: 0.610\n",
      "[2,   650] loss: 0.598\n",
      "[2,   700] loss: 0.608\n",
      "[2,   750] loss: 0.601\n",
      "[2,   800] loss: 0.612\n",
      "[2,   850] loss: 0.589\n",
      "[2,   900] loss: 0.584\n",
      "[2,   950] loss: 0.618\n",
      "[2,  1000] loss: 0.581\n",
      "[2,  1050] loss: 0.578\n",
      "[2,  1100] loss: 0.595\n",
      "[2,  1150] loss: 0.607\n",
      "[2,  1200] loss: 0.601\n",
      "[2,  1250] loss: 0.595\n",
      "[2,  1300] loss: 0.592\n",
      "[2,  1350] loss: 0.609\n",
      "[2,  1400] loss: 0.594\n",
      "[2,  1450] loss: 0.591\n",
      "[2,  1500] loss: 0.607\n",
      "[2,  1550] loss: 0.580\n",
      "[2,  1600] loss: 0.590\n",
      "[2,  1650] loss: 0.574\n",
      "[2,  1700] loss: 0.580\n",
      "[2,  1750] loss: 0.582\n",
      "[2,  1800] loss: 0.603\n",
      "[2,  1850] loss: 0.595\n",
      "Accuracy of the network on the test images: 0.130\n",
      "[3,    50] loss: 0.579\n",
      "[3,   100] loss: 0.573\n",
      "[3,   150] loss: 0.574\n",
      "[3,   200] loss: 0.583\n",
      "[3,   250] loss: 0.568\n",
      "[3,   300] loss: 0.598\n",
      "[3,   350] loss: 0.577\n",
      "[3,   400] loss: 0.568\n",
      "[3,   450] loss: 0.579\n",
      "[3,   500] loss: 0.573\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8151df3ef767>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mmlp_EKF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melmnt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetWeightsgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_EKF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dle\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dle\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_every = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch,(inputs,labels) in enumerate(trainloader,0):\n",
    "        loss=[]\n",
    "        H=[]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #print(\"Shape of Inputs\",inputs.shape)\n",
    "        inputs= inputs.view(-1,784)\n",
    "        labels= labels.to(dtype=torch.long)\n",
    "        outputs = mlp_EKF(inputs)\n",
    "        #print(\"Shape of Outputs\",outputs.shape)\n",
    "        #For every element in batch\n",
    "        for elmnt in range(outputs.shape[0]):\n",
    "            label_vector=torch.zeros(outputs.shape[1]).to(device)\n",
    "            label_vector[labels] = 1\n",
    "            err= torch.sqrt(cross_err_loss(outputs[elmnt],label_vector))\n",
    "            #print(\"Error at element\", elmnt, err)\n",
    "            loss.append(err) \n",
    "            for k in range(outputs.shape[1]):\n",
    "                mlp_EKF.zero_grad()\n",
    "                outputs[elmnt][k].backward(retain_graph=True)\n",
    "                H.append(getWeightsgrad(mlp_EKF).to(device).view(-1))\n",
    "                                \n",
    "        loss=torch.cat(loss).view(-1,1)\n",
    "        net_loss=torch.mean(loss)\n",
    "        H=torch.stack(H,dim=0)\n",
    "        #print(H)\n",
    "        Ak = torch.inverse(R + torch.mm(torch.mm(H, P), torch.transpose(H,0,1)))\n",
    "        Kk = torch.mm(torch.mm(P, torch.transpose(H,0,1)), Ak)\n",
    "        weight_mat= getWeights(mlp_EKF).to(device)\n",
    "        weight_mat = weight_mat + torch.mm(Kk, loss)\n",
    "        setWeights(mlp_EKF,weight_mat)\n",
    "        P = P + Q - torch.mm(torch.mm(Kk,H),P)\n",
    "        \n",
    "        # Print loss for every batch \n",
    "        #print(\"Loss for batch\", batch + 1, net_loss)\n",
    "        # print statistics\n",
    "        running_loss += net_loss.item()\n",
    "        #display.clear_output(wait=True)\n",
    "        if (batch % print_every) == (print_every-1):\n",
    "            #display.clear_output(wait=True)\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, batch+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "#         if skip_training:\n",
    "#             break\n",
    "\n",
    "#     # Print accuracy after every epoch\n",
    "    accuracy = compute_accuracy(mlp_EKF, testloader)\n",
    "    print('Accuracy of the network on the test images: %.3f' % accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
